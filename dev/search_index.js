var documenterSearchIndex = {"docs":
[{"location":"#Methode*of*conjugate_gradients.jl","page":"Methodeofconjugate_gradients.jl","title":"Methodeofconjugate_gradients.jl","text":"","category":"section"},{"location":"","page":"Methodeofconjugate_gradients.jl","title":"Methodeofconjugate_gradients.jl","text":"This is the documentation for the repository of the method of Conjugate Gradients.jl. The documentation is divided into two parts. The first part provides a mathematical explanation of Conjugate Gradients and Gradient Descent. The second part provides a detailed explanation of the code.","category":"page"},{"location":"#The-mathematical-explanation","page":"Methodeofconjugate_gradients.jl","title":"The mathematical explanation","text":"","category":"section"},{"location":"#Graph-Embedding-using-Conjugate-Gradient-Method","page":"Methodeofconjugate_gradients.jl","title":"Graph Embedding using Conjugate Gradient Method","text":"","category":"section"},{"location":"","page":"Methodeofconjugate_gradients.jl","title":"Methodeofconjugate_gradients.jl","text":"In the context of graph embedding, the conjugate gradient method can be applied to find an equilibrium position for the nodes in a plane or space using a physical method. The idea is to treat the graph as a system of masses connected by springs, where the goal is to find the equilibrium positions of the nodes that minimize the total potential energy of the system. The potential energy function can be defined based on the distance between neighboring nodes.","category":"page"},{"location":"","page":"Methodeofconjugate_gradients.jl","title":"Methodeofconjugate_gradients.jl","text":"The equation given for each coordinate (xi, yi, zi) of the vertices of the graph in space is:","category":"page"},{"location":"","page":"Methodeofconjugate_gradients.jl","title":"Methodeofconjugate_gradients.jl","text":"$","category":"page"},{"location":"","page":"Methodeofconjugate_gradients.jl","title":"Methodeofconjugate_gradients.jl","text":"\\begin{aligned} -st(i) xi + \\sum{j \\in N(i)} xj - st(i) yi + \\sum{j \\in N(i)} yj &= 0, \\\n-st(i) yi + \\sum{j \\in N(i)} yj - st(i) zi + \\sum{j \\in N(i)} zj &= 0, \\\n-st(i) zi + \\sum{j \\in N(i)} zj - st(i) xi + \\sum{j \\in N(i)} xj &= 0. \\end{aligned} $","category":"page"},{"location":"","page":"Methodeofconjugate_gradients.jl","title":"Methodeofconjugate_gradients.jl","text":"Here, st(i) represents the stage of the i-th node, and N(i) is the set of indices of neighboring nodes. This equation ensures that the total force acting on each node is zero in equilibrium. If some nodes are fixed, the others will occupy an equilibrium position between the fixed nodes.","category":"page"},{"location":"","page":"Methodeofconjugate_gradients.jl","title":"Methodeofconjugate_gradients.jl","text":"To solve this system of equations using the conjugate gradient method, we can first rewrite the equations in matrix form Ax = b, where A is a sparse matrix representing the graph structure, and x and b are vectors containing the coordinates of the vertices and the right-hand side of the equation, respectively. The conjugate gradient method can then be used to solve this linear system iteratively, finding the equilibrium positions of the nodes in the graph.","category":"page"},{"location":"","page":"Methodeofconjugate_gradients.jl","title":"Methodeofconjugate_gradients.jl","text":"The example provided in the code section demonstrates how to create a sparse matrix representation of the graph embedding problem, and then use the conj_grad function to solve it. ","category":"page"},{"location":"#Conjugate-gradient-method","page":"Methodeofconjugate_gradients.jl","title":"Conjugate gradient method","text":"","category":"section"},{"location":"","page":"Methodeofconjugate_gradients.jl","title":"Methodeofconjugate_gradients.jl","text":"The conjugate gradient method is an iterative algorithm used to solve linear systems of equations, particularly for symmetric and positive-definite matrices. It converges faster than other iterative methods like gradient descent, especially when dealing with large and sparse matrices.","category":"page"},{"location":"#The-Gradient-descent-method","page":"Methodeofconjugate_gradients.jl","title":"The Gradient descent method","text":"","category":"section"},{"location":"","page":"Methodeofconjugate_gradients.jl","title":"Methodeofconjugate_gradients.jl","text":"Gradient descent is an optimization algorithm that tries to minimize a given objective function by iteratively moving in the direction of the steepest descent, as defined by the negative of the gradient. In this code, gradient descent is applied to a 2D quadratic function for visualization purposes.","category":"page"},{"location":"#The-code-explaind","page":"Methodeofconjugate_gradients.jl","title":"The code explaind","text":"","category":"section"},{"location":"","page":"Methodeofconjugate_gradients.jl","title":"Methodeofconjugate_gradients.jl","text":"This code imports three Julia packages: LinearAlgebra, Plots.\nusing LinearAlgebra  using Plots\nThe code defines a ScatteredArray structure with two matrices and provides custom multiplication and size functions for it. This data structure is specifically designed to represent sparse matrices efficiently.\n struct ScatteredArray\n     V::Matrix{Float64}\n     I::Matrix{Int}\n end\n\n # Multiplication function for ScatteredArray\n function Base.:*(A::ScatteredArray, x::Vector{Float64})\n     result = zeros(size(A.I, 1))\n\n     for row in 1:size(A.I, 1)\n         for col in 1:size(A.I, 2)\n             i = A.I[row, col]\n             if i != 0 && i <= length(x)\n                 result[row] += A.V[row, col] * x[i]\n             end\n         end\n     end\n\n     return result\n end\nThe code defines a function to create a ScatteredArray from an adjacency matrix and a vector of strengths. This function is essential for creating the sparse matrix representation of the graph embedding problem.\n function create_scattered_system_matrix(adj_matrix::Matrix{Int},st::Vector{Float64})\n     n = size(adj_matrix, 1)\n     A = zeros(Float64, n, n)\n\n     for i in 1:n\n         A[i, i] = st[i] * sum(adj_matrix[i, :]) - 1\n         for j in 1:n\n             if adj_matrix[i, j] == 1\n                 A[i, j] = -1\n             end\n         end\n     end\n\n     # Add a small diagonal perturbation\n     A = A + 1e-6 * Matrix{Float64}(LinearAlgebra.I, n, n)\n\n     V = zeros(Float64, n, n)\n     I = zeros(Int, n, n)\n\n     for i in 1:n\n         k = 1\n         for j in 1:n\n             if A[i, j] != 0\n                 V[i, k] = A[i, j]\n                 I[i, k] = j\n                 k += 1\n             end\n         end\n     end\n     return ScatteredArray(V, I)\n end\nThis code defines a conj_grad function that solves a linear system Ax = b using the conjugate gradient method\n function conj_grad(A::ScatteredArray, b::Vector{Float64})\n n = length(b)\n x = zeros(Float64, n)\n r = b - A * x\n p = r\n rsold = r' * r\n max_iter = 1000\n\n     for i = 1:max_iter\n         Ap = A * p\n         alpha = rsold / (p' * Ap)\n         x = x + alpha * p\n         r = r - alpha * Ap\n         rsnew = r' * rs\n\n         if sqrt(rsnew) < 1e-10\n             break\n         end\n\n         p = r + (rsnew / rsold) * p\n         rsold = rsnew\n     end\n return x, i\n end","category":"page"},{"location":"","page":"Methodeofconjugate_gradients.jl","title":"Methodeofconjugate_gradients.jl","text":"The code demonstrates how to use the conj_grad function to solve the graph embedding problem using a physical method. The example is a simple graph with a few nodes and edges. The adjacency matrix and the strengths vector are given.","category":"page"},{"location":"","page":"Methodeofconjugate_gradients.jl","title":"Methodeofconjugate_gradients.jl","text":"# Example graph\nadj_matrix = [\n    0 1 0 0 1;\n    1 0 1 0 1;\n    0 1 0 1 0;\n    0 0 1 0 1;\n    1 1 0 1 0\n]\n\nst = [1.0, 1.0, 1.0, 1.0, 1.0]\n\nA = create_scattered_system_matrix(adj_matrix, st)\nb = [0.0, 0.0, 0.0, 0.0, 0.0]\n\nx, iterations = conj_grad(A, b)\n\nprintln(\"Solution: \", x)\nprintln(\"Number of iterations: \", iterations)","category":"page"},{"location":"","page":"Methodeofconjugate_gradients.jl","title":"Methodeofconjugate_gradients.jl","text":"The output shows the solution vector x and the number of iterations required to achieve convergence.","category":"page"},{"location":"","page":"Methodeofconjugate_gradients.jl","title":"Methodeofconjugate_gradients.jl","text":"To summarize, the code provides a complete implementation of the Conjugate Gradient method for solving sparse linear systems with ScatteredArray data type. This is particularly useful for the task of graph embedding using the physical method, as demonstrated in the provided example.","category":"page"},{"location":"","page":"Methodeofconjugate_gradients.jl","title":"Methodeofconjugate_gradients.jl","text":"The code provided below solves a quadratic optimization problem using both the gradient descent and the conjugate gradient methods. The problem is defined as minimizing the function f(x) = 0.5 * x' * A * x - b' * x, where A and b are given. The contour plot shows the convergence of both methods. The gradient descent method converges to a local minimum, while the conjugate gradient method converges to the global minimum. The code also shows how to use the Plots package to create a contour plot.\n function f(x)\n     return 0.5 * x' * A * x - b' * x\n end\n\n function grad_f(x)\n     return A * x - b\n end\n\n function gradient_descent(grad_f, x0, max_iter=1000, tol=1e-6, lr=0.1)\n     x = x0\n     path = [x0]\n\n     for i in 1:max_iter\n         g = grad_f(x)\n         x = x - lr * g\n         push!(path, x)\n\n         if norm(grad_f(x)) < tol\n             break\n         end\n     end\n\n     return x, path\n end\n\n function conj_grad_2d(A, b, x0, max_iter=1000, tol=1e-6)\n     x = x0\n     r = b - A * x\n     p = copy(r)\n\n     path = [x0]\n\n     for i in 1:max_iter\n         alpha = dot(r, r) / dot(p, A * p)\n         x = x + alpha * p\n         push!(path, x)\n\n         r_new = r - alpha * A * p\n\n         if norm(r_new) < tol\n             break\n         end\n\n         beta = dot(r_new, r_new) / dot(r, r)\n         p = r_new + beta * p\n         r = r_new\n     end\n\n     return x, path\n end\n\n x0 = [2.0; 2.0]\n sol_gd, path_gd = gradient_descent(grad_f, x0)\n sol_cg, path_cg = conj_grad_2d(A, b, x0)\n\n x = -1:0.1:3\n y = -1:0.1:3\n contour_plot = Plots.contour(x, y, (x, y) -> f([x; y]), title=\"Gradient Descent vs Conjugate Gradient\", xlabel=\"x\", ylabel=\"y\", legend=:topleft, color=:black, linewidth=0.5)\n\n x_coords_gd = [p[1] for p in path_gd]\n y_coords_gd = [p[2] for p in path_gd]\n plot!(contour_plot, x_coords_gd, y_coords_gd, marker=:circle, color=:green, lw=1.5, markersize=4, label=\"Gradient Descent\")\n\n x_coords_cg = [p[1] for p in path_cg]\n y_coords_cg = [p[2] for p in path_cg]\n plot!(contour_plot, x_coords_cg, y_coords_cg, marker=:circle, color=:red, lw=1.5, markersize=4, label=\"Conjugate Gradient\")\n\n plot!(contour_plot)","category":"page"}]
}
